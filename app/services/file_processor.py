"""
æª”æ¡ˆè™•ç†æœå‹™ - è™•ç†ä¸Šå‚³æª”æ¡ˆçš„å®Œæ•´æµç¨‹
"""

import asyncio
import shutil
from pathlib import Path
from typing import Dict, List
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.file import File, FileStatus
from app.services.file_storage import file_storage
from app.utils.document_converter import DocumentConverter
from app.utils.summarizer import SummaryProcessor
from app.utils.embedding_processor import EmbeddingProcessor
from app.models.llm.ollama_client import OllamaClient


class FileProcessingService:
    """æª”æ¡ˆè™•ç†æœå‹™ - è² è²¬å®Œæ•´çš„è™•ç†æµç¨‹"""
    
    def __init__(self):
        self.converter = DocumentConverter()
        self.ollama_client = OllamaClient()
        self.summarizer = SummaryProcessor(self.ollama_client)
        self.embedder = EmbeddingProcessor(self.ollama_client)
        self.last_temp_dir = None  # ä¿å­˜æœ€è¿‘ä¸€æ¬¡çš„æš«å­˜ç›®éŒ„è·¯å¾‘
    
    async def process_files_batch(
        self,
        file_ids: List[int],
        task_id: str,
        db: AsyncSession,
        progress_callback = None
    ) -> Dict:
        """
        æ‰¹æ¬¡è™•ç†æª”æ¡ˆ
        
        åƒæ•¸:
            file_ids: æª”æ¡ˆ ID åˆ—è¡¨
            task_id: ä»»å‹™ IDï¼ˆç”¨æ–¼æ›´æ–°å‰ç«¯é€²åº¦ï¼‰
            db: è³‡æ–™åº« session
            progress_callback: é€²åº¦å›å‘¼å‡½æ•¸
            
        è¿”å›:
            è™•ç†çµæœçµ±è¨ˆ
        """
        results = {
            'total': len(file_ids),
            'success': 0,
            'failed': 0,
            'errors': []
        }
        
        for idx, file_id in enumerate(file_ids):
            try:
                # ç²å–æª”æ¡ˆè¨˜éŒ„
                file_record = await db.get(File, file_id)
                if not file_record:
                    results['failed'] += 1
                    results['errors'].append(f"æª”æ¡ˆ ID {file_id} ä¸å­˜åœ¨")
                    continue
                
                # æ›´æ–°ç‹€æ…‹ç‚ºè™•ç†ä¸­
                file_record.status = FileStatus.PROCESSING
                file_record.processing_step = "classify"
                file_record.processing_progress = 0
                await db.commit()
                
                # åŸ·è¡Œå››éšæ®µè™•ç†
                success = await self._process_single_file(file_record, db, progress_callback)
                
                if success:
                    file_record.status = FileStatus.COMPLETED
                    file_record.processing_step = "completed"
                    file_record.processing_progress = 100
                    results['success'] += 1
                else:
                    file_record.status = FileStatus.FAILED
                    results['failed'] += 1
                
                await db.commit()
                
            except Exception as e:
                results['failed'] += 1
                results['errors'].append(f"æª”æ¡ˆ ID {file_id}: {str(e)}")
                
                # æ›´æ–°ç‚ºå¤±æ•—ç‹€æ…‹
                try:
                    file_record = await db.get(File, file_id)
                    if file_record:
                        file_record.status = FileStatus.FAILED
                        file_record.error_message = str(e)
                        await db.commit()
                except:
                    pass
        
        return results
    
    async def _process_single_file(
        self,
        file_record: File,
        db: AsyncSession,
        progress_callback = None
    ) -> bool:
        """
        è™•ç†å–®ä¸€æª”æ¡ˆçš„å››éšæ®µæµç¨‹
        ä½¿ç”¨æš«å­˜è³‡æ–™å¤¾ï¼Œå…¨éƒ¨æˆåŠŸæ‰ç§»å‹•åˆ°æ­£ç¢ºä½ç½®
        
        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸ
        """
        import tempfile
        temp_dir = None
        
        try:
            file_path = Path(file_record.file_path)
            department_id = file_record.department_id
            
            # æ¸…ç†ä¸Šä¸€æ¬¡çš„æš«å­˜ç›®éŒ„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self.last_temp_dir and Path(self.last_temp_dir).exists():
                try:
                    shutil.rmtree(self.last_temp_dir)
                    print(f"ğŸ—‘ï¸ å·²æ¸…ç†ä¸Šæ¬¡æš«å­˜ç›®éŒ„: {self.last_temp_dir}")
                except Exception as e:
                    print(f"âš ï¸ æ¸…ç†ä¸Šæ¬¡æš«å­˜ç›®éŒ„å¤±æ•—: {e}")
            
            # å‰µå»ºæ–°çš„æš«å­˜ç›®éŒ„
            temp_dir = Path(tempfile.mkdtemp(prefix="rag_process_"))
            self.last_temp_dir = str(temp_dir)  # ä¿å­˜è·¯å¾‘
            print(f"\nğŸ—‚ï¸ ä½¿ç”¨æš«å­˜ç›®éŒ„: {temp_dir}")
            print(f"ğŸ’¡ æ­¤æš«å­˜ç›®éŒ„å°‡ä¿ç•™åˆ°ä¸‹æ¬¡è™•ç†æ™‚æ‰æ¸…ç†")
            
            # éšæ®µ 1: æº–å‚™è™•ç† (0-25%)
            print(f"\nğŸ“‚ éšæ®µ 1: æº–å‚™è™•ç† - {file_record.original_filename}")
            file_record.processing_step = "classify"
            file_record.processing_progress = 10
            await db.commit()
            
            # è¤‡è£½æª”æ¡ˆåˆ°æš«å­˜ç›®éŒ„ï¼ˆä¸ç§»å‹•åŸæª”æ¡ˆï¼‰
            temp_data_file = temp_dir / "data" / file_path.name
            temp_data_file.parent.mkdir(parents=True, exist_ok=True)
            await asyncio.to_thread(shutil.copy2, str(file_path), str(temp_data_file))
            
            file_record.processing_progress = 25
            await db.commit()
            
            # éšæ®µ 2: è½‰æ›ç‚º Markdown (25-50%)
            print(f"\nğŸ“ éšæ®µ 2: è½‰æ›ç‚º Markdown")
            file_record.processing_step = "convert"
            file_record.processing_progress = 30
            await db.commit()
            
            temp_md_path = temp_dir / "output_md" / f"{file_path.stem}.md"
            temp_md_path.parent.mkdir(parents=True, exist_ok=True)
            
            success = await asyncio.to_thread(
                self.converter.convert_to_markdown,
                temp_data_file,
                temp_md_path,
                use_mineru_for_pdf=True
            )
            
            if not success:
                file_record.error_message = "Markdown è½‰æ›å¤±æ•—"
                raise Exception("Markdown è½‰æ›å¤±æ•—")
            
            file_record.processing_progress = 50
            await db.commit()
            
            # éšæ®µ 3: ç”Ÿæˆæ‘˜è¦ (50-75%)
            print(f"\nğŸ’¡ éšæ®µ 3: ç”Ÿæˆæ‘˜è¦")
            file_record.processing_step = "summarize"
            file_record.processing_progress = 55
            await db.commit()
            
            temp_summary_path = temp_dir / "summaries" / f"{file_path.stem}_summary.json"
            temp_summary_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨æ–°çš„summarizerï¼Œæœƒè‡ªå‹•è™•ç†é•·æ–‡æª”åˆ†å¡Š
            success = await asyncio.to_thread(
                self.summarizer.process_markdown_file,
                temp_md_path,
                temp_summary_path
            )
            
            if not success:
                file_record.error_message = "æ‘˜è¦ç”Ÿæˆå¤±æ•—"
                raise Exception("æ‘˜è¦ç”Ÿæˆå¤±æ•—")
            
            file_record.processing_progress = 75
            await db.commit()
            
            # éšæ®µ 4: ç”ŸæˆåµŒå…¥ (75-100%)
            print(f"\nğŸ”¢ éšæ®µ 4: ç”Ÿæˆå‘é‡åµŒå…¥")
            file_record.processing_step = "embed"
            file_record.processing_progress = 80
            await db.commit()
            
            # è™•ç†ä¸»æ‘˜è¦çš„åµŒå…¥
            temp_embedding_path = temp_dir / "embeddings" / f"{file_path.stem}_embedding.json"
            temp_embedding_path.parent.mkdir(parents=True, exist_ok=True)
            
            success = await asyncio.to_thread(
                self.embedder.process_summary_file,
                temp_summary_path,
                temp_embedding_path
            )
            
            if not success:
                file_record.error_message = "åµŒå…¥ç”Ÿæˆå¤±æ•—"
                raise Exception("åµŒå…¥ç”Ÿæˆå¤±æ•—")
            
            # æª¢æŸ¥ä¸¦è™•ç†åˆ†å¡Šæ‘˜è¦çš„åµŒå…¥
            additional_embedding_files = []
            summary_dir = temp_summary_path.parent
            base_name = file_path.stem
            
            # å‹•æ…‹å°‹æ‰¾æ‰€æœ‰åˆ†å¡Šæ‘˜è¦æª”æ¡ˆ
            i = 2
            while True:
                part_summary_file = summary_dir / f"{base_name}_part{i}_summary.json"
                if not part_summary_file.exists():
                    break  # æ²’æœ‰æ›´å¤šåˆ†å¡Š
                    
                part_embedding_file = temp_dir / "embeddings" / f"{base_name}_part{i}_embedding.json"
                
                success = await asyncio.to_thread(
                    self.embedder.process_summary_file,
                    part_summary_file,
                    part_embedding_file
                )
                
                if success:
                    additional_embedding_files.append(part_embedding_file)
                    print(f"      âœ… åˆ†å¡Š {i} åµŒå…¥å®Œæˆ")
                    
                i += 1
            
            if additional_embedding_files:
                print(f"    ğŸ”¢ è™•ç†äº† {len(additional_embedding_files)} å€‹åˆ†å¡Šçš„åµŒå…¥")
            
            file_record.processing_progress = 90
            await db.commit()
            
            # æ‰€æœ‰éšæ®µæˆåŠŸï¼Œç§»å‹•æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®
            print(f"\nğŸ“¦ ç§»å‹•æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®...")
            
            # ç§»å‹•åˆ° processed/data
            final_data_path = file_storage._get_processed_path(department_id, "data") / file_path.name
            await asyncio.to_thread(shutil.move, str(temp_data_file), str(final_data_path))
            
            # ç§»å‹• markdown
            final_md_path = file_storage._get_processed_path(department_id, "output_md") / temp_md_path.name
            await asyncio.to_thread(shutil.move, str(temp_md_path), str(final_md_path))
            
            # ç§»å‹•ä¸»æ‘˜è¦
            final_summary_path = file_storage._get_processed_path(department_id, "summaries") / temp_summary_path.name
            await asyncio.to_thread(shutil.move, str(temp_summary_path), str(final_summary_path))
            
            # ç§»å‹•åˆ†å¡Šæ‘˜è¦æª”æ¡ˆ
            additional_summary_files = []
            summary_dir = temp_summary_path.parent
            base_name = file_path.stem
            
            i = 2
            while True:
                part_summary_file = summary_dir / f"{base_name}_part{i}_summary.json"
                if not part_summary_file.exists():
                    break
                    
                final_part_summary = file_storage._get_processed_path(department_id, "summaries") / part_summary_file.name
                await asyncio.to_thread(shutil.move, str(part_summary_file), str(final_part_summary))
                additional_summary_files.append(final_part_summary)
                print(f"    ğŸ“„ ç§»å‹•åˆ†å¡Šæ‘˜è¦: {part_summary_file.name}")
                i += 1
            
            # ç§»å‹•ä¸»åµŒå…¥
            final_embedding_path = file_storage._get_processed_path(department_id, "embeddings") / temp_embedding_path.name
            await asyncio.to_thread(shutil.move, str(temp_embedding_path), str(final_embedding_path))
            
            # ç§»å‹•åˆ†å¡ŠåµŒå…¥æª”æ¡ˆ
            moved_embedding_count = 0
            for embedding_file in additional_embedding_files:
                if embedding_file.exists():
                    final_part_embedding = file_storage._get_processed_path(department_id, "embeddings") / embedding_file.name
                    await asyncio.to_thread(shutil.move, str(embedding_file), str(final_part_embedding))
                    moved_embedding_count += 1
                    print(f"    ğŸ”¢ ç§»å‹•åˆ†å¡ŠåµŒå…¥: {embedding_file.name}")
            
            # è¨ˆç®—ç¸½çš„chunkå’Œvectoræ•¸é‡
            total_chunks = 1 + len(additional_summary_files)
            total_vectors = 1 + moved_embedding_count
            
            # æ›´æ–°è³‡æ–™åº«è¨˜éŒ„
            file_record.file_path = str(final_data_path)
            file_record.markdown_path = str(final_md_path)
            file_record.summary_path = str(final_summary_path)
            file_record.embedding_path = str(final_embedding_path)
            file_record.is_vectorized = True
            file_record.chunk_count = total_chunks
            file_record.vector_count = total_vectors
            file_record.processing_progress = 100
            await db.commit()
            
            print(f"âœ… æª”æ¡ˆè™•ç†å®Œæˆ: {file_record.original_filename}")
            if total_chunks > 1:
                print(f"    ğŸ“„ ç”Ÿæˆäº† {total_chunks} å€‹åˆ†å¡Šæ‘˜è¦å’Œ {total_vectors} å€‹å‘é‡åµŒå…¥")
            
            # åˆªé™¤ unprocessed ä¸­çš„åŸå§‹æª”æ¡ˆ
            if file_path.exists() and 'unprocessed' in str(file_path):
                try:
                    file_path.unlink()
                    print(f"ğŸ—‘ï¸ å·²åˆªé™¤åŸå§‹æª”æ¡ˆ: {file_path}")
                except Exception as e:
                    print(f"âš ï¸ åˆªé™¤åŸå§‹æª”æ¡ˆå¤±æ•—: {e}")
            
            # ä¿ç•™æš«å­˜ç›®éŒ„ä¾›æª¢æŸ¥ï¼ˆä¸‹æ¬¡è™•ç†æ™‚æ‰æ¸…ç†ï¼‰
            print(f"ğŸ“ æš«å­˜ç›®éŒ„å·²ä¿ç•™: {temp_dir}")
            print(f"   å¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹: ls -la {temp_dir}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ è™•ç†å¤±æ•—: {e}")
            file_record.error_message = str(e)
            
            # å¤±æ•—æ™‚æ¸…ç†ç•¶å‰çš„æš«å­˜ç›®éŒ„
            if temp_dir and temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir)
                    print(f"ğŸ—‘ï¸ å·²æ¸…ç†æš«å­˜ç›®éŒ„ï¼ˆè™•ç†å¤±æ•—ï¼‰")
                    self.last_temp_dir = None  # æ¸…é™¤è¨˜éŒ„
                except Exception as cleanup_error:
                    print(f"âš ï¸ æ¸…ç†æš«å­˜ç›®éŒ„å¤±æ•—: {cleanup_error}")
            
            return False


# å»ºç«‹å…¨åŸŸå¯¦ä¾‹
file_processing_service = FileProcessingService()
