"""æª”æ¡ˆå„²å­˜æœå‹™ (File Storage Service)"""

import os
import uuid
import aiofiles
import shutil
from datetime import datetime
from pathlib import Path
from typing import BinaryIO, Optional
from fastapi import UploadFile, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession

from app.config import settings
from app.services.system_settings import system_settings_service


class FileStorageService:
    """æª”æ¡ˆå„²å­˜æœå‹™"""

    def __init__(self):
        """åˆå§‹åŒ–æª”æ¡ˆå„²å­˜æœå‹™"""
        self.base_path = Path(settings.UPLOAD_DIR)
        self.base_path.mkdir(parents=True, exist_ok=True)

    def _get_department_path(self, department_id: int, subdirectory: str = "unprocessed") -> Path:
        """å–å¾—è™•å®¤çš„æª”æ¡ˆå„²å­˜è·¯å¾‘
        
        Args:
            department_id: è™•å®¤ ID
            subdirectory: å­ç›®éŒ„åç¨± (unprocessed æˆ– processed)
        """
        dept_path = self.base_path / str(department_id) / subdirectory
        dept_path.mkdir(parents=True, exist_ok=True)
        return dept_path
    
    def _get_processed_path(self, department_id: int, process_type: str) -> Path:
        """å–å¾—è™•ç†å¾Œæª”æ¡ˆçš„è·¯å¾‘
        
        Args:
            department_id: è™•å®¤ ID
            process_type: è™•ç†é¡å‹ (data, output_md, summaries, embeddings)
        """
        processed_path = self.base_path / str(department_id) / "processed" / process_type
        processed_path.mkdir(parents=True, exist_ok=True)
        return processed_path

    def generate_unique_filename(self, original_filename: str) -> str:
        """ç”Ÿæˆå”¯ä¸€æª”å
        
        æ ¼å¼: åŸæª”å (å¦‚æœé‡è¤‡å‰‡åŠ ä¸Šæ™‚é–“æˆ³è¨˜)
        ä¾‹å¦‚: äººäº‹è¦ç« .pdf æˆ– äººäº‹è¦ç« _20251113_143000.pdf
        """
        # åˆ†é›¢æª”åå’Œå‰¯æª”å
        name, ext = os.path.splitext(original_filename)
        
        # æ¸…ç†æª”åä¸­çš„ç‰¹æ®Šå­—å…ƒ
        safe_name = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_', '.')).strip()
        
        # åŸºæœ¬æª”å
        base_filename = f"{safe_name}{ext}"
        
        # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼ŒåŠ ä¸Šæ™‚é–“æˆ³è¨˜
        # é€™å€‹æª¢æŸ¥æœƒåœ¨ save_upload_file ä¸­é€²è¡Œ
        return base_filename

    async def save_upload_file(
        self,
        upload_file: UploadFile,
        department_id: int
    ) -> tuple[str, str, int]:
        """å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆåˆ° unprocessed ç›®éŒ„
        
        Args:
            upload_file: FastAPI UploadFile ç‰©ä»¶
            department_id: è™•å®¤ ID
            
        Returns:
            tuple: (unique_filename, file_path, file_size)
        """
        # ç”Ÿæˆæª”å
        unique_filename = self.generate_unique_filename(upload_file.filename)
        
        # å–å¾— unprocessed å„²å­˜è·¯å¾‘
        dept_path = self._get_department_path(department_id, "unprocessed")
        file_path = dept_path / unique_filename
        
        # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼ŒåŠ ä¸Šæ™‚é–“æˆ³è¨˜é¿å…è¡çª
        if file_path.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            name, ext = os.path.splitext(unique_filename)
            unique_filename = f"{name}_{timestamp}{ext}"
            file_path = dept_path / unique_filename
        
        # å„²å­˜æª”æ¡ˆ
        file_size = 0
        async with aiofiles.open(file_path, 'wb') as f:
            # åˆ†å¡Šè®€å–å’Œå¯«å…¥ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º
            chunk_size = 1024 * 1024  # 1MB
            while chunk := await upload_file.read(chunk_size):
                await f.write(chunk)
                file_size += len(chunk)
        
        return unique_filename, str(file_path), file_size
    
    def move_to_processed(
        self,
        source_path: str,
        department_id: int,
        process_type: str = "data"
    ) -> str:
        """å°‡æª”æ¡ˆå¾ unprocessed ç§»å‹•åˆ° processed ç›®éŒ„
        
        Args:
            source_path: ä¾†æºæª”æ¡ˆè·¯å¾‘
            department_id: è™•å®¤ ID
            process_type: è™•ç†é¡å‹ (data, output_md, summaries, embeddings)
            
        Returns:
            str: æ–°çš„æª”æ¡ˆè·¯å¾‘
        """
        source = Path(source_path)
        if not source.exists():
            raise FileNotFoundError(f"ä¾†æºæª”æ¡ˆä¸å­˜åœ¨: {source_path}")
        
        # å–å¾—ç›®æ¨™è·¯å¾‘
        target_dir = self._get_processed_path(department_id, process_type)
        target_path = target_dir / source.name
        
        # ç§»å‹•æª”æ¡ˆ
        shutil.move(str(source), str(target_path))
        
        return str(target_path)

    def delete_file(self, file_path: str) -> bool:
        """åˆªé™¤æª”æ¡ˆ
        
        Args:
            file_path: æª”æ¡ˆå®Œæ•´è·¯å¾‘
            
        Returns:
            bool: æ˜¯å¦åˆªé™¤æˆåŠŸ
        """
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                return True
            return False
        except Exception as e:
            print(f"åˆªé™¤æª”æ¡ˆå¤±æ•—: {file_path}, éŒ¯èª¤: {str(e)}")
            return False
    
    def delete_file_completely(self, file_record, department_id: int) -> dict:
        """å®Œæ•´åˆªé™¤æª”æ¡ˆåŠå…¶æ‰€æœ‰ç›¸é—œæª”æ¡ˆ
        
        åŒ…æ‹¬ï¼š
        - åŸå§‹æª”æ¡ˆ
        - Markdown è½‰æ›æª”æ¡ˆ
        - æ‘˜è¦æª”æ¡ˆï¼ˆåŒ…æ‹¬åˆ†å¡Šæª”æ¡ˆ part1, part2, etc.ï¼‰
        - åµŒå…¥å‘é‡æª”æ¡ˆï¼ˆåŒ…æ‹¬åˆ†å¡Šæª”æ¡ˆï¼‰
        
        Args:
            file_record: æª”æ¡ˆè¨˜éŒ„ç‰©ä»¶
            department_id: è™•å®¤ ID
            
        Returns:
            dict: æ¸…ç†çµæœçµ±è¨ˆ
        """
        cleanup_stats = {
            'original_file': False,
            'markdown_file': False,
            'summary_files': 0,
            'embedding_files': 0,
            'errors': []
        }
        
        try:
            # å–å¾—æª”æ¡ˆåŸºæœ¬è³‡è¨Š
            original_filename = file_record.original_filename
            file_path = file_record.file_path
            
            # å¾å¯¦éš›æª”æ¡ˆè·¯å¾‘æ¨æ–·æª”åä¸»å¹¹ï¼Œè€Œä¸æ˜¯åƒ…å¾ original_filename
            # å› ç‚ºè™•ç†å¾Œçš„æª”æ¡ˆå¯èƒ½æœ‰æ™‚é–“æˆ³å¾Œç¶´
            if file_path and Path(file_path).exists():
                # å¾æª”æ¡ˆè·¯å¾‘å–å¾—å¯¦éš›æª”åä¸»å¹¹
                filename_stem = Path(file_path).stem
            else:
                # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå˜—è©¦å¾ original_filename æ¨æ–·
                filename_stem = Path(original_filename).stem
            
            # ç‰¹åˆ¥è™•ç†ï¼šå¦‚æœåœ¨ processed ç›®éŒ„ä¸­æ‰¾ä¸åˆ°ä»¥ filename_stem å‘½åçš„æª”æ¡ˆï¼Œ
            # å˜—è©¦æŸ¥æ‰¾åŒ…å«åŸå§‹æª”åï¼ˆå»æ‰å‰¯æª”åï¼‰çš„æª”æ¡ˆ
            processed_path = self._get_department_path(department_id, "processed")
            
            # å…ˆç”¨åŸå§‹æ–¹æ³•æŸ¥æ‰¾
            test_summary = processed_path / "summaries" / f"{filename_stem}_summary.json"
            test_embedding = processed_path / "embeddings" / f"{filename_stem}_embedding.json"
            
            if not test_summary.exists() and not test_embedding.exists():
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå˜—è©¦åœ¨ç›®éŒ„ä¸­æœå°‹åŒ…å«åŸå§‹æª”åçš„æª”æ¡ˆ
                original_stem = Path(original_filename).stem
                summary_dir = processed_path / "summaries"
                
                if summary_dir.exists():
                    # æœå°‹ä»¥åŸå§‹æª”åé–‹é ­çš„æ‘˜è¦æª”æ¡ˆï¼Œå„ªå…ˆæ‰¾ä¸»æª”æ¡ˆï¼ˆä¸å« _partï¼‰
                    matching_files = list(summary_dir.glob(f"{original_stem}*_summary.json"))
                    if matching_files:
                        # å„ªå…ˆé¸æ“‡ä¸»æª”æ¡ˆï¼ˆä¸å« _part çš„ï¼‰
                        main_files = [f for f in matching_files if "_part" not in f.stem]
                        if main_files:
                            # å¾ä¸»æª”æ¡ˆæ¨æ–·å¯¦éš›çš„æª”åä¸»å¹¹
                            actual_filename = main_files[0].stem.replace("_summary", "")
                            filename_stem = actual_filename
                            print(f"ğŸ” å¾ä¸»æª”æ¡ˆæ¨æ–·æª”åä¸»å¹¹: {filename_stem}")
                        else:
                            # å¦‚æœæ²’æœ‰ä¸»æª”æ¡ˆï¼Œå¾åˆ†å¡Šæª”æ¡ˆæ¨æ–·
                            actual_filename = matching_files[0].stem.replace("_summary", "")
                            # ç§»é™¤ _part éƒ¨åˆ†ï¼Œç²å¾—åŸºæœ¬æª”å
                            if "_part" in actual_filename:
                                filename_stem = actual_filename.rsplit("_part", 1)[0]
                            else:
                                filename_stem = actual_filename
                            print(f"ğŸ” å¾åˆ†å¡Šæª”æ¡ˆæ¨æ–·æª”åä¸»å¹¹: {filename_stem}")
            
            print(f"ğŸ“‚ ä½¿ç”¨æª”åä¸»å¹¹é€²è¡Œæ¸…ç†: {filename_stem}")
            print(f"ğŸ“‚ åŸå§‹æª”å: {original_filename}")
            
            # å–å¾—è™•å®¤è·¯å¾‘
            dept_path = self._get_department_path(department_id)
            processed_path = self._get_department_path(department_id, "processed")
            
            # 1. åˆªé™¤åŸå§‹æª”æ¡ˆ
            if file_record.file_path and os.path.exists(file_record.file_path):
                try:
                    os.remove(file_record.file_path)
                    cleanup_stats['original_file'] = True
                    print(f"âœ… å·²åˆªé™¤åŸå§‹æª”æ¡ˆ: {file_record.file_path}")
                except Exception as e:
                    cleanup_stats['errors'].append(f"åˆªé™¤åŸå§‹æª”æ¡ˆå¤±æ•—: {str(e)}")
            
            # 2. åˆªé™¤ Markdown æª”æ¡ˆ
            markdown_file = processed_path / "output_md" / f"{filename_stem}.md"
            if markdown_file.exists():
                try:
                    markdown_file.unlink()
                    cleanup_stats['markdown_file'] = True
                    print(f"âœ… å·²åˆªé™¤ Markdown æª”æ¡ˆ: {markdown_file}")
                except Exception as e:
                    cleanup_stats['errors'].append(f"åˆªé™¤ Markdown æª”æ¡ˆå¤±æ•—: {str(e)}")
            
            # 3. åˆªé™¤æ‘˜è¦æª”æ¡ˆï¼ˆåŒ…æ‹¬åˆ†å¡Šæª”æ¡ˆï¼‰
            summary_dir = processed_path / "summaries"
            if summary_dir.exists():
                # ä¸»æ‘˜è¦æª”æ¡ˆ
                main_summary = summary_dir / f"{filename_stem}_summary.json"
                if main_summary.exists():
                    try:
                        main_summary.unlink()
                        cleanup_stats['summary_files'] += 1
                        print(f"âœ… å·²åˆªé™¤ä¸»æ‘˜è¦æª”æ¡ˆ: {main_summary}")
                    except Exception as e:
                        cleanup_stats['errors'].append(f"åˆªé™¤ä¸»æ‘˜è¦æª”æ¡ˆå¤±æ•—: {str(e)}")
                
                # åˆ†å¡Šæ‘˜è¦æª”æ¡ˆï¼ˆpart2, part3, ...ï¼‰
                for part_file in summary_dir.glob(f"{filename_stem}_part*_summary.json"):
                    try:
                        part_file.unlink()
                        cleanup_stats['summary_files'] += 1
                        print(f"âœ… å·²åˆªé™¤åˆ†å¡Šæ‘˜è¦æª”æ¡ˆ: {part_file}")
                    except Exception as e:
                        cleanup_stats['errors'].append(f"åˆªé™¤åˆ†å¡Šæ‘˜è¦æª”æ¡ˆå¤±æ•—: {str(e)}")
            
            # 4. åˆªé™¤åµŒå…¥å‘é‡æª”æ¡ˆï¼ˆåŒ…æ‹¬åˆ†å¡Šæª”æ¡ˆï¼‰
            embeddings_dir = processed_path / "embeddings"
            if embeddings_dir.exists():
                # ä¸»åµŒå…¥æª”æ¡ˆï¼ˆå¯èƒ½æ˜¯ _embedding.json æˆ– _embeddings.jsonï¼‰
                for pattern in [f"{filename_stem}_embedding.json", f"{filename_stem}_embeddings.json"]:
                    main_embedding = embeddings_dir / pattern
                    if main_embedding.exists():
                        try:
                            main_embedding.unlink()
                            cleanup_stats['embedding_files'] += 1
                            print(f"âœ… å·²åˆªé™¤ä¸»åµŒå…¥æª”æ¡ˆ: {main_embedding}")
                        except Exception as e:
                            cleanup_stats['errors'].append(f"åˆªé™¤ä¸»åµŒå…¥æª”æ¡ˆå¤±æ•—: {str(e)}")
                
                # åˆ†å¡ŠåµŒå…¥æª”æ¡ˆï¼ˆpart2, part3, ...ï¼‰
                for part_file in embeddings_dir.glob(f"{filename_stem}_part*_embedding.json"):
                    try:
                        part_file.unlink()
                        cleanup_stats['embedding_files'] += 1
                        print(f"âœ… å·²åˆªé™¤åˆ†å¡ŠåµŒå…¥æª”æ¡ˆ: {part_file}")
                    except Exception as e:
                        cleanup_stats['errors'].append(f"åˆªé™¤åˆ†å¡ŠåµŒå…¥æª”æ¡ˆå¤±æ•—: {str(e)}")
                
                # ä¹Ÿè™•ç†å¯èƒ½çš„ _embeddings.json æ ¼å¼
                for part_file in embeddings_dir.glob(f"{filename_stem}_part*_embeddings.json"):
                    try:
                        part_file.unlink()
                        cleanup_stats['embedding_files'] += 1
                        print(f"âœ… å·²åˆªé™¤åˆ†å¡ŠåµŒå…¥æª”æ¡ˆ: {part_file}")
                    except Exception as e:
                        cleanup_stats['errors'].append(f"åˆªé™¤åˆ†å¡ŠåµŒå…¥æª”æ¡ˆå¤±æ•—: {str(e)}")
            
            # 5. åˆªé™¤å…¶ä»–å¯èƒ½çš„è¡ç”Ÿæª”æ¡ˆ
            # æª¢æŸ¥ data ç›®éŒ„
            data_dir = processed_path / "data"
            if data_dir.exists():
                for data_file in data_dir.glob(f"{filename_stem}.*"):
                    try:
                        data_file.unlink()
                        print(f"âœ… å·²åˆªé™¤è³‡æ–™æª”æ¡ˆ: {data_file}")
                    except Exception as e:
                        cleanup_stats['errors'].append(f"åˆªé™¤è³‡æ–™æª”æ¡ˆå¤±æ•—: {str(e)}")
            
            print(f"ğŸ—‘ï¸ æª”æ¡ˆæ¸…ç†å®Œæˆ: {original_filename}")
            print(f"   - åŸå§‹æª”æ¡ˆ: {'âœ…' if cleanup_stats['original_file'] else 'âŒ'}")
            print(f"   - Markdown: {'âœ…' if cleanup_stats['markdown_file'] else 'âŒ'}")
            print(f"   - æ‘˜è¦æª”æ¡ˆ: {cleanup_stats['summary_files']} å€‹")
            print(f"   - åµŒå…¥æª”æ¡ˆ: {cleanup_stats['embedding_files']} å€‹")
            if cleanup_stats['errors']:
                print(f"   - éŒ¯èª¤: {len(cleanup_stats['errors'])} å€‹")
            
            return cleanup_stats
            
        except Exception as e:
            error_msg = f"æª”æ¡ˆæ¸…ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            cleanup_stats['errors'].append(error_msg)
            print(f"âŒ {error_msg}")
            return cleanup_stats

    def get_file_size(self, file_path: str) -> int:
        """å–å¾—æª”æ¡ˆå¤§å°ï¼ˆbytesï¼‰"""
        if os.path.exists(file_path):
            return os.path.getsize(file_path)
        return 0

    async def validate_file(
        self, 
        upload_file: UploadFile,
        db: AsyncSession
    ) -> tuple[bool, Optional[str]]:
        """é©—è­‰æª”æ¡ˆ
        
        Args:
            upload_file: ä¸Šå‚³çš„æª”æ¡ˆ
            db: è³‡æ–™åº« session
            
        Returns:
            tuple: (is_valid, error_message)
        """
        # å¾è³‡æ–™åº«å–å¾—æª”æ¡ˆå¤§å°é™åˆ¶
        max_file_size = await system_settings_service.get_max_file_size(db)
        
        # æª¢æŸ¥æª”æ¡ˆå¤§å°
        if hasattr(upload_file, 'size') and upload_file.size:
            if upload_file.size > max_file_size:
                return False, f"æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ ({max_file_size / (1024**2):.0f} MB)"
        
        # å¾è³‡æ–™åº«å–å¾—å…è¨±çš„æª”æ¡ˆé¡å‹
        allowed_exts = await system_settings_service.get_allowed_file_types(db)
        
        # æª¢æŸ¥æª”æ¡ˆé¡å‹
        ext = os.path.splitext(upload_file.filename)[1].lower()
        if ext not in allowed_exts:
            return False, f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {ext}ï¼Œå…è¨±çš„æ ¼å¼: {', '.join(allowed_exts)}"
        
        return True, None

    def get_file_info(self, file_path: str) -> dict:
        """å–å¾—æª”æ¡ˆè³‡è¨Š"""
        if not os.path.exists(file_path):
            return None
        
        stat = os.stat(file_path)
        return {
            "size": stat.st_size,
            "created": datetime.fromtimestamp(stat.st_ctime),
            "modified": datetime.fromtimestamp(stat.st_mtime),
            "exists": True
        }

    def get_storage_stats(self, department_id: Optional[int] = None) -> dict:
        """å–å¾—å„²å­˜ç©ºé–“çµ±è¨ˆ
        
        Args:
            department_id: è™•å®¤ IDï¼ˆå¯é¸ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰è™•å®¤ï¼‰
            
        Returns:
            dict: å„²å­˜çµ±è¨ˆè³‡è¨Š
        """
        if department_id:
            path = self._get_department_path(department_id)
        else:
            path = self.base_path
        
        total_size = 0
        file_count = 0
        
        for root, dirs, files in os.walk(path):
            for file in files:
                file_path = os.path.join(root, file)
                if os.path.exists(file_path):
                    total_size += os.path.getsize(file_path)
                    file_count += 1
        
        return {
            "total_size": total_size,
            "file_count": file_count,
            "total_size_mb": round(total_size / (1024**2), 2),
            "total_size_gb": round(total_size / (1024**3), 2)
        }


# å»ºç«‹å…¨åŸŸæª”æ¡ˆå„²å­˜æœå‹™å¯¦ä¾‹
file_storage = FileStorageService()
